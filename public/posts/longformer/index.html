<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>LongFormer : The Long Document Transformer | Mr. KnowNothing</title>
<meta name="keywords" content="Transformer, Long Sequences, LLM">
<meta name="description" content="Describes the working of Longformer , an encoder based Transformer model for Long Sequences">
<meta name="author" content="Tanul Singh">
<link rel="canonical" href="https://tanulsingh.github.io/public/posts/longformer/">
<link crossorigin="anonymous" href="/public/assets/css/stylesheet.56b565077d1dbf909913170abd9566246db7edc878d350e8854aa758936ad458.css" integrity="sha256-VrVlB30dv5CZExcKvZVmJG237ch401DohUqnWJNq1Fg=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://tanulsingh.github.io/public/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://tanulsingh.github.io/public/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://tanulsingh.github.io/public/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://tanulsingh.github.io/public/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://tanulsingh.github.io/public/posts/longformer/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="LongFormer : The Long Document Transformer" />
<meta property="og:description" content="Describes the working of Longformer , an encoder based Transformer model for Long Sequences" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tanulsingh.github.io/public/posts/longformer/" />
<meta property="og:image" content="https://tanulsingh.github.io/public/longformer.jpg" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-02-12T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-02-12T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://tanulsingh.github.io/public/longformer.jpg" />
<meta name="twitter:title" content="LongFormer : The Long Document Transformer"/>
<meta name="twitter:description" content="Describes the working of Longformer , an encoder based Transformer model for Long Sequences"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Courses",
      "item": "https://tanulsingh.github.io/public/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "LongFormer : The Long Document Transformer",
      "item": "https://tanulsingh.github.io/public/posts/longformer/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LongFormer : The Long Document Transformer",
  "name": "LongFormer : The Long Document Transformer",
  "description": "Describes the working of Longformer , an encoder based Transformer model for Long Sequences",
  "keywords": [
    "Transformer", "Long Sequences", "LLM"
  ],
  "articleBody": "Transformer-Based Models have become the go to models in about every NLP task since their inception, but when it comes to long documents they suffer from a drawback of limited tokens . Transformer-Based Models are unable to process long sequences due to their self attention which scales quadratically with the sequence length . Longformer addresses this limitation and proposes an attention mechanism which scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer’s attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention (more on this later in this article) .\nIn this article we will deep dive into the Longformer Paper and look at all of its components in depth , exploring the results and techniques to use it efficiently.\nWhat Happenend Before LongFormer? This is a question that would have come into your mind , that if not Longformer then what ? Before the LongFormer or even now , people often used chunking strategy , wherein , a long document is chunked into 512 token chunks with or without an overlapping window and are processed separately . While it is a good work around for Long Documents , this approach suffer from information loss due to truncation or cascading errors , especially when doing NER or Span Extraction where we need the whole context to understand and mark the labels correctly . Some other Task specific workarounds have also been explored , like sparse attention that defines some form of sparse attention pattern and avoids computing the full quadratic attention matrix multiplication . LongFormer tries to adopt the second method and presents a new SOTA for Long Documents\nLongFormer Attention : Components LongFormer proposes a sparsified form of self-attention , wherein ,they sparsify the full self-attention matrix according to an “attention pattern” specifying pairs of input locations attending to one another. Unlike the full self-attention, the proposed attention pattern scales linearly with the input sequence, making it efficient for longer sequences. Let’s look into the components of the Attention Pattern .\nSliding Window Attention A full-fledged Self-Attention allows the model to look at all the tokens in the input sentence but comes with a computational complexity . Below is an example of Full Self-Attention\nIn an attempt to reduce computational complexity without losing local context , Longformer proposes fixed-size window attention surrounding each token. Given a fixed window size w, each token attends to 1/2w tokens on each side as shown in figure below\nSince there are n encoder layers stacked on top of each other in a typical Transformer-Base Network, using multiple stacked layers of such windowed attention results in a large receptive field, where top layers have access to all input locations and have the capacity to build representations that incorporate information across the entire input, similar to CNNs (see figure below) . Thus even though any token in the first layer can only see only w/2 tokens on both sides at the time , the topmost layers can see effectively look at L*W tokens , a simple example is shown in the example below\nThe computation complexity of this pattern is O(n × w), which scales linearly with input sequence length n. Even though this attention pattern results in a high receptive field at the top layers , one can tweak the attention width depending upon the type of problem being solved and compute power to increase the receptive field even further and guage more context .\nDilated Sliding Window Attention To further increase the receptive field without increasing computation, the sliding window can be “dilated” . This is analogous to dilated CNNs where the window has gaps of size dilation d . In the similar example as above we can have a window attention with dilation d that means we will attend to words with a difference of d within a window w+d , thereby increasing the number of tokens the model can see at a time . Assuming a fixed d and w for all layers, the receptive field is L × d × w, which can reach tens of thousands of tokens even for small values of d.\nGlobal Attention While the windowed attention solves the problem of complexity and captures local context , it still lacks the flexibility to learn task-specific representations . Longformer thus allows few tokens to attend globally in a symmetric way that is, a token with a global attention attends to all tokens across the sequence, and all tokens in the sequence attend to it. For example for classification, global attention is used for the [CLS] token while in QA global attention is provided on all question tokens. While specifying global attention is task specific, it is a easy way to add inductive bias to the model’s attention.\nLonFormer Attention : Implementation Now that we have a good understanding of all components of LongFormer Attention and its pattern , we will look at its implementation. Transformer model computes attention scores as follows:\n$Attention(Q, K, V ) = softmax(QKT√dk) V$ (1)\nLongformer uses two sets of projections, Qs, Ks, Vs to compute attention scores of sliding window attention, and Qg, Kg, Vg to compute attention scores for the global attention. The additional projections provide flexibility to model the different types of attention, which is critical for best performance on downstream tasks. In Equation (1) ,The expensive operation is the matrix multiplication QKT because both Q and K have n (sequence length) projections. For Longformer, the dilated sliding window attention computes only a fixed number of the diagonals of QKT. This results in a linear increase in memory usage compared to quadratic increase for full self-attention.\nNote : LongFormer Attention Pattern can be used as a drop in replacement and can be plucked into any pretrained transformer model without the need to change the model architecture .\nTraining Procedure According to the authors , one of their main motivations is to develop a Pretrained Model which can be suitably used for downstream tasks for long documents and is suitable . To do so they pretrain the Longformer model on a document corpus and finetune it for six tasks, including classification, QA and coreference resolution. The resulting model can process sequences up to 4,096 tokens long (8 times longer than BERT)\nTraining Objective They pretrain Longformer with masked language modeling (MLM) objective similar to BERT, where the goal is to recover randomly masked tokens in a sequence. Since MLM pretraining is expensive, they continue pretraining from the RoBERTa released checkpoint, while only making the minimal changes necessary to support Longformer’s attention mechanism\nAttention Pattern They use sliding window attention with window size of 512, therefore using the same amount of computation as RoBERTa.\nPosition Embeddings RoBERTa uses learned absolute position embeddings with the maximum position being 512. To support longer documents, they add extra position embeddings to support up to position 4,096. To leverage RoBERTa’s pretrained weights, instead of randomly initializing the new position embeddings, they initialize them by copying the 512 position embeddings from RoBERTa multiple times\nContinued MLM PreTraining The Authors pretrain Longformer using fairseq on a corpus of long documents that they compiled. They train two model sizes, a base model and a large model. Both models are trained for 65K gradient updates with sequences length 4,096, batch size 64 (218 tokens), maximum learning rate of 3e-5, linear warmup of 500 steps, followed by a power 3 polynomial decay. The rest of the hyperparameters are the same as RoBERTa.\nResults The Authors apply the trained Longformer on multiple long document tasks, including QA, coreference resolution and classification . Below Table contains the results for each of the three tasks . Longformer Base beats Roberta-base in each of the three long document tasks and achieves new SOTA for long documents . Its performance gain is especially obvious for tasks that require long context such as WikiHop and Hyperpartisan.\nThere are some tricks that the author have used to increase the performance for LongFormer\nDifferential Attention Window : As explained every layer in LongFormer only looks at the tokens in the attention window , given that there is performance vs compute tradeoff by increasing or decreasing those attention window . The LongFormer authors use large attention window on the top layers and bottom attention window on the lower layers , the intuition being bottom layer mine the syntactic and semantic information while the top layers mine task specific information Global Attention : Global attention helps the LongFormer to learn the inductive bias for the task , hence choosing the tokens to attend to globally can really enhance performance .The authors demonstrate this by using global attention on Questions in Q/A Task and on CLF token in Classification Task. Conclusion LongFormer effectively presents an Attention Mechanism which not only scales linearly with sequence length but also beats SOTA Roberta model on Long Document Tasks . Although the Global Attention of Longformer remains task specific , its a fair trade given the results and optimizations. LongFormer attention Pattern gives a solid starting point to be used with other models as well like XLNet, MPnet ,etc.\nIf you have enjoyed the blog, I will recommend reading the original paper.\n",
  "wordCount" : "1547",
  "inLanguage": "en",
  "image":"https://tanulsingh.github.io/public/longformer.jpg","datePublished": "2022-02-12T00:00:00Z",
  "dateModified": "2022-02-12T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Tanul Singh"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://tanulsingh.github.io/public/posts/longformer/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Mr. KnowNothing",
    "logo": {
      "@type": "ImageObject",
      "url": "https://tanulsingh.github.io/public/favicon.ico"
    }
  }
}
</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
            {left: "\\begin{align}", right: "\\end{align}", display: true},
            {left: "\\begin{align*}", right: "\\end{align*}", display: true},
            {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
            {left: "\\begin{gather}", right: "\\end{gather}", display: true},
            {left: "\\begin{CD}", right: "\\end{CD}", display: true},
          ],
          throwOnError : false
        });
    });
</script>
 


</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://tanulsingh.github.io/public/" accesskey="h" title="Tanul Singh">
                <img src="https://tanulsingh.github.io/public/402A2908.JPG" alt="" aria-label="logo"
                    height="20"
                    width="20">Tanul Singh</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://tanulsingh.github.io/public/cv.pdf" title="CV">
                    <span>CV</span>
                </a>
            </li>
            <li>
                <a href="https://tanulsingh.github.io/public/posts/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="https://tanulsingh.github.io/public/projects/" title="Personal Projects">
                    <span>Personal Projects</span>
                </a>
            </li>
            <li>
                <a href="https://tanulsingh.github.io/public/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://tanulsingh.github.io/public/archive/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      LongFormer : The Long Document Transformer
    </h1>
    <div class="post-meta"><span title='2022-02-12 00:00:00 +0000 UTC'>February 2022</span>&nbsp;&middot;&nbsp;8 min&nbsp;&middot;&nbsp;Tanul Singh
</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#what-happenend-before-longformer">What Happenend Before LongFormer?</a></li>
    <li><a href="#longformer-attention--components">LongFormer Attention : Components</a></li>
    <li><a href="#lonformer-attention--implementation">LonFormer Attention : Implementation</a></li>
    <li><a href="#training-procedure">Training Procedure</a></li>
    <li><a href="#results">Results</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><p><strong>Transformer-Based Models</strong> have become the go to models in about every NLP task since their inception, but when it comes to long documents they suffer from a drawback of <strong>limited tokens</strong> . Transformer-Based Models are unable to process long sequences due to their self attention which scales quadratically with the sequence length .  <strong>Longformer addresses this limitation and proposes an attention mechanism which scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer</strong>.  Longformer’s attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention (more on this later in this article) .</p>
<p>In this article we will deep dive into the <strong>Longformer Paper</strong> and look at all of its components in depth , exploring the results and techniques to use it efficiently.</p>
<p><img loading="lazy" src="longformer.jpg" alt="Longformer"  />
</p>
<h2 id="what-happenend-before-longformer">What Happenend Before LongFormer?<a hidden class="anchor" aria-hidden="true" href="#what-happenend-before-longformer">#</a></h2>
<p>This is a question that would have come into your mind , that if not Longformer then what ? <em><strong>Before the LongFormer</strong></em> or even now , people often used <em><strong>chunking strategy</strong></em> , wherein , a long document is chunked into 512 token chunks with or without an overlapping window and are processed separately . While it is a good work around for Long Documents , this approach suffer from information loss due to truncation or cascading errors , especially when doing NER or Span Extraction where we need the whole context to understand and mark the labels correctly . Some other Task specific workarounds have also been explored , like sparse attention that defines some form of sparse attention pattern and avoids computing the full quadratic attention
matrix multiplication . LongFormer tries to adopt the second method and presents a new SOTA for Long Documents</p>
<h2 id="longformer-attention--components">LongFormer Attention : Components<a hidden class="anchor" aria-hidden="true" href="#longformer-attention--components">#</a></h2>
<p><img loading="lazy" src="attention.png" alt="Attention"  />
</p>
<p>LongFormer proposes a sparsified form of self-attention , wherein ,they sparsify the full self-attention matrix according to an “attention pattern” specifying pairs of input locations attending to one another. Unlike the full self-attention, the proposed attention pattern scales linearly with the input sequence, making it efficient for longer sequences. Let’s look into the components of the <strong>Attention Pattern</strong> .</p>
<h3 id="sliding-window-attention">Sliding Window Attention<a hidden class="anchor" aria-hidden="true" href="#sliding-window-attention">#</a></h3>
<p>A full-fledged Self-Attention allows the model to look at all the tokens in the input sentence but comes with a computational complexity . Below is an example of Full Self-Attention</p>
<p><img loading="lazy" src="full_attention.jpg" alt="Full Fledged Attention"  />
</p>
<p>In an attempt to reduce computational complexity without losing local context , Longformer proposes fixed-size window attention surrounding each token. <strong>Given a fixed window size w, each token attends to 1/2w tokens on each side</strong> as shown in figure below</p>
<p><img loading="lazy" src="sliding_window.jpg" alt="Sliding Window Attention"  />
</p>
<p>Since there are n encoder layers stacked on top of each other in a typical Transformer-Base Network, using multiple stacked layers of such windowed attention results in a large receptive field, where top layers have access to all input locations and have the capacity to build representations that incorporate information across the entire input, similar to CNNs (see figure below) . Thus even though any token in the first layer can only see only w/2 tokens on both sides at the time , the topmost layers can see effectively look at L*W tokens , a simple example is shown in the example below</p>
<p><img loading="lazy" src="receptive_field.jpg" alt="Recepetive Field"  />
</p>
<p>The computation complexity of this pattern is O(n × w), which scales linearly with input sequence length n. <em>Even though this attention pattern results in a high receptive field at the top layers , one can tweak the attention width depending upon the type of problem being solved and compute power to increase the receptive field even further and guage more context</em> .</p>
<h3 id="dilated-sliding-window-attention">Dilated Sliding Window Attention<a hidden class="anchor" aria-hidden="true" href="#dilated-sliding-window-attention">#</a></h3>
<p>To further increase the receptive field without increasing computation, <em><strong>the sliding window can be “dilated”</strong></em> . This is analogous to dilated CNNs where the window has gaps of size dilation d . In the similar example as above we can have a window attention with dilation d that means we will attend to words with a difference of d within a window w+d , thereby increasing the number of tokens the model can see at a time . Assuming a fixed d and w for all layers, the receptive field is L × d × w, which can reach tens of thousands of tokens even for small values of d.</p>
<h3 id="global-attention">Global Attention<a hidden class="anchor" aria-hidden="true" href="#global-attention">#</a></h3>
<p>While the windowed attention solves the problem of complexity and captures local context , it still lacks the flexibility to learn task-specific representations . Longformer thus allows few tokens to attend globally in a symmetric way that is, a token with a global attention attends to all tokens across the sequence, and all tokens in the sequence attend to it. For example for classification, global attention is used for the [CLS] token while in QA global attention is provided on all question tokens. While specifying global attention is task specific, it is a easy way to add inductive bias to the model’s attention.</p>
<h2 id="lonformer-attention--implementation">LonFormer Attention : Implementation<a hidden class="anchor" aria-hidden="true" href="#lonformer-attention--implementation">#</a></h2>
<p>Now that we have a good understanding of all components of LongFormer Attention and its pattern , we will look at its implementation. Transformer model computes attention scores as follows:</p>
<p>$Attention(Q, K, V ) = softmax(QKT√dk)
V$ (1)</p>
<p>Longformer uses two sets of projections, Qs, Ks, Vs to compute attention scores of sliding window attention, and Qg, Kg, Vg to compute attention scores for the global attention. The additional projections provide flexibility to model the different types of attention, which is critical for best performance on downstream tasks. In Equation (1) ,The expensive operation is the matrix multiplication QKT because both Q and K have n (sequence length) projections. For Longformer, the dilated sliding window attention computes only a fixed number of the diagonals of QKT. This results in a linear increase in memory usage compared to quadratic increase for full self-attention.</p>
<blockquote>
<p>Note : LongFormer Attention Pattern can be used as a drop in replacement and can be plucked into any pretrained transformer model without the need to change the model architecture .</p>
</blockquote>
<h2 id="training-procedure">Training Procedure<a hidden class="anchor" aria-hidden="true" href="#training-procedure">#</a></h2>
<p>According to the authors , one of their main motivations is to develop a Pretrained Model which can be suitably used for downstream tasks for long documents and is suitable . To do so they pretrain the Longformer model on a document corpus and finetune it for six tasks, including classification, QA and coreference resolution. The resulting model can process sequences up to 4,096 tokens long (8 times longer than BERT)</p>
<h3 id="training-objective">Training Objective<a hidden class="anchor" aria-hidden="true" href="#training-objective">#</a></h3>
<p>They pretrain Longformer with masked language modeling (MLM) objective similar to BERT, where the goal is to recover randomly masked tokens in a sequence. Since MLM pretraining is expensive, they continue pretraining from the RoBERTa released checkpoint, while only making the minimal changes necessary to support Longformer’s attention mechanism</p>
<h3 id="attention-pattern">Attention Pattern<a hidden class="anchor" aria-hidden="true" href="#attention-pattern">#</a></h3>
<p>They use sliding window attention with window size of 512, therefore using the
same amount of computation as RoBERTa.</p>
<h3 id="position-embeddings">Position Embeddings<a hidden class="anchor" aria-hidden="true" href="#position-embeddings">#</a></h3>
<p>RoBERTa uses learned absolute position embeddings with the maximum position being 512. To support longer documents, they add extra position embeddings to support up to position 4,096. To leverage RoBERTa’s pretrained weights, instead of randomly initializing the new position embeddings, <em><strong>they initialize them by copying the 512 position embeddings from RoBERTa multiple times</strong></em></p>
<h3 id="continued-mlm-pretraining">Continued MLM PreTraining<a hidden class="anchor" aria-hidden="true" href="#continued-mlm-pretraining">#</a></h3>
<p>The Authors pretrain Longformer using fairseq on a corpus of long documents that they compiled. They train two model sizes, a base model and a large model. Both models are trained for 65K gradient updates with sequences length 4,096, batch size 64 (218 tokens), maximum learning rate of 3e-5, linear warmup of 500 steps, followed by a power 3 polynomial decay. The rest of the hyperparameters are the same as RoBERTa.</p>
<h2 id="results">Results<a hidden class="anchor" aria-hidden="true" href="#results">#</a></h2>
<p>The Authors apply the trained <strong>Longformer</strong> on multiple long document tasks, including QA, coreference resolution and classification . Below Table contains the results for each of the three tasks . Longformer Base beats Roberta-base in each of the three long document tasks and achieves new SOTA for long documents . Its performance gain is especially obvious for tasks that require long context such as WikiHop and Hyperpartisan.</p>
<p><img loading="lazy" src="longformerresults.png" alt="Results"  />
</p>
<p>There are some tricks that the author have used to increase the performance for LongFormer</p>
<ul>
<li><strong>Differential Attention Window</strong> : As explained every layer in LongFormer only looks at the tokens in the attention window , given that there is performance vs compute tradeoff by increasing or decreasing those attention window . The LongFormer authors use large attention window on the top layers and bottom attention window on the lower layers , the intuition being bottom layer mine the syntactic and semantic information while the top layers mine task specific information</li>
<li><strong>Global Attention</strong> : Global attention helps the LongFormer to learn the inductive bias for the task , hence choosing the tokens to attend to globally can really enhance performance .The authors demonstrate this by using global attention on Questions in Q/A Task and on CLF token in Classification Task.</li>
</ul>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>LongFormer effectively presents an Attention Mechanism which not only scales linearly with sequence length but also beats SOTA Roberta model on Long Document Tasks . Although the Global Attention of Longformer remains task specific , its a fair trade given the results and optimizations. LongFormer attention Pattern gives a solid starting point to be used with other models as well like XLNet, MPnet ,etc.</p>
<p>If you have enjoyed the blog, I will recommend reading the original paper.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://tanulsingh.github.io/public/tags/transformer/">Transformer</a></li>
      <li><a href="https://tanulsingh.github.io/public/tags/long-sequences/">Long Sequences</a></li>
      <li><a href="https://tanulsingh.github.io/public/tags/llm/">LLM</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://tanulsingh.github.io/public/">Mr. KnowNothing</a></span> ·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">a modified version</a>
         of 
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
